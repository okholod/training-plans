# Machine Learning Engineer / Python

As a **Machine Learning Engineer**, you will build, optimize and deploy machine learning models to production. 
You’ll generally be treating machine learning models as APIs or components, which you’ll be plugging into a full-stack app or 
hardware of some kind, but you may also be called upon to design models yourself.

**Required skills:**
- Python programming skills
- Solid foundation in SQL
- Numpy, scikit-learn, Pandas
- Enterprise deep learning frameworks: TensorFlow/PyTorch
- Cloud containerization environments and micro-services: Docker, Kubernetes
- Experience working with SQL and noSQL databases using Python APIs: IBM DB2, IBM DashDB, Cloudant
-	Understanding of REST principles and experience with web services frameworks like Flask
- Experience working with Apache Spark

**Desired skills:**
- Stream-processing software: Kafka, RabbitMQ, MQTT, Apache Spark Streaming
-	Experience with web scraping frameworks
- Experience with NLP frameworks: SpaCy, NLTK

## Jupyter Notebooks
- Start with installing Anaconda and check Jupyter notebooks there - https://www.anaconda.com/distribution/
- Read the user documentation and play with samples - https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/examples_index.html
- Jupyter Notebook Tips, Tricks, and Shortcuts - https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/

## Data Science Basics
- Start with these learning paths and courses
  - Data Science Foundations - https://cognitiveclass.ai/learn/data-science/
  - Applied Data Science with Python - https://cognitiveclass.ai/learn/data-science-with-python/
  - Machine Learning with Python - https://cognitiveclass.ai/courses/machine-learning-with-python/
  
## Docker + Kubernetes
- Quick introduction videos
  - Learn Docker in 12 Minutes - https://www.youtube.com/watch?v=YFl2mCHdv24
  - Docker Compose in 12 Minutes - https://www.youtube.com/watch?v=Qw9zlE3t8Ko
- Complete the following courses
  - Docker Essentials: A Developer Introduction - https://cognitiveclass.ai/courses/docker-essentials/
  - Container & Kubernetes Essentials with IBM Cloud - https://cognitiveclass.ai/courses/kubernetes-course/
- Look at the best practices
  - Best practices for writing Dockerfiles - https://docs.docker.com/develop/develop-images/dockerfile_best-practices/

### Minikube
- Install Minikube on your workstation
  - Use the following guide to install kubectl https://kubernetes.io/docs/tasks/tools/install-kubectl/
  - Use the following guide to install minikube https://kubernetes.io/docs/tasks/tools/install-minikube/
- Complete the following tutorial https://kubernetes.io/docs/tutorials/kubernetes-basics/

## Big Data Foundations
- Complete the following course to get essential understanding about Big Data and Hadoop
  - Big Data and Hadoop Essentials - https://www.udemy.com/big-data-and-hadoop-essentials-free-tutorial
  
## Apache Spark / PySpark
- Quick orientation on what Apache Spark is
  - Apache Spark: A Conceptual Orientation - https://towardsdatascience.com/apache-spark-a-conceptual-orientation-e326f8c57a64
- Complete the following course on Spark
  - Big Data Analytics Using Spark - https://www.edx.org/course/big-data-analytics-using-spark

### ETL with PySpark
- Look at these tutorials for simple ETL jobs in PySpark
  - How to create a simple ETL Job locally with PySpark, PostgreSQL and Docker - https://itnext.io/how-to-create-a-simple-etl-job-locally-with-pyspark-postgresql-and-docker-ea53cd43311d
  - Create your first ETL Pipeline in Apache Spark and Python - https://towardsdatascience.com/create-your-first-etl-pipeline-in-apache-spark-and-python-ec3d12e2c169

## IBM Cloud + Apache Spark
- Create IBM Cloud account - https://cloud.ibm.com/registration
- Create an instance of Analytics Engine service - https://cloud.ibm.com/catalog/services/analytics-engine

### Training Application - Data Load
- Create an account on Github and share it with your supervisor
- Create an instance of DB2 service on the cloud
- Create a Python application that
  - Connects to the DB2 instance on the cloud
  - Generates a table with 20k of sample records
    - *product_id* - autogenerated numeric
    - *product_group* - autogenerated numeric in 0..9 range
    - *year* - autogenerated numeric in 2015..2018 range
    - 12 columns with monthly purchases amount - numeric in 0..100000 range
  - There should be no duplicates for the same product/year
  - If there are multiple product rows for different years, product/group combination should be concise
- Use environment variables to specify configuration parameters like DB URL and connection credentials
- Add description of the application functionality and instructions on how to run it into a *README.md* file in Github

### Training Application - Data Transformation
- Create a data tranformation application using Apache Spark
- Read data from the DB2 on the cloud (use the service instance and data from the previous task) 
- Aggregate data in the dataframe by calculating total purchases amount per year
  - For each row calculate total of monthly purchases
  - Save the year total as a new column *year_purchases*
  - Remove the columns with monthly amounts from the data frame
- Save the modified dataframe as a file in Cloud Object Storage - https://www.ibm.com/cloud/object-storage
- Application should be published to Github
- Use *spark-submit* to run the application and Spark configuration properties to specify configuration parameters like DB URL and connection credentials
  - Submitting Applications - https://spark.apache.org/docs/latest/submitting-applications.html
  - Method to get Spark configuration properties - https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/SparkContext.html#getConf--
- Add description of the application functionality and instructions on how to run it using *spark-submit* into a *README.md* file in Github 

### Training Application - Unit tests
  - Start with this article - Unittesting Apache Spark Applications / A PySpark case - https://towardsdatascience.com/unittesting-apache-spark-applications-b9a46e319ce3- Add unit tests to the training application
  - Add tests into your application
  
### Spark + Kubernetes
- Study the Apache Spark documentation
  - Running Spark on Kubernetes - https://spark.apache.org/docs/latest/running-on-kubernetes.html
- Run your application on minikube

## Apache Airflow
- Familiarize yourself with Apache Airflow 
  - Documentation - https://airflow.apache.org/
  - Getting started with Apache Airflow - https://towardsdatascience.com/getting-started-with-apache-airflow-df1aa77d7b1b
  - Airflow tutorial videos - https://www.youtube.com/playlist?list=PLYizQ5FvN6pvIOcOd6dFZu3lQqc6zBGp2
  - Airflow: Lesser Known Tips, Tricks, and Best Practises - https://medium.com/datareply/airflow-lesser-known-tips-tricks-and-best-practises-cf4d4a90f8f
- Create a sample DAG via UI for a Spark submit and run it
- Create a Python script defining the same DAG programatically via Airflow API and run it on Airflow

For quick overview of how Airflow clusters work look at these two articles:
- How Apache Airflow Distributes Jobs on Celery workers - https://blog.sicara.com/using-airflow-with-celery-workers-54cb5212d405
- Setting up an Apache Airflow Cluster - http://site.clairvoyantsoft.com/setting-apache-airflow-cluster/

## IBM AI Engineering Professional Certificate
- Complete all courses in the IBM AI Engineering Professional Certificate program on Coursera - https://www.coursera.org/professional-certificates/ai-engineer
