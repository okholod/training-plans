# Data Engineering + Scala

## Big Data Foundations
- https://www.udemy.com/big-data-and-hadoop-essentials-free-tutorial

## Scala + Apache Spark
- https://cognitiveclass.ai/learn/scala/
- https://www.udemy.com/scala-and-spark-2-getting-started

## IBM Cloud + Apache Spark
- Create IBM Cloud account - https://cloud.ibm.com/registration
- Create an instance of Analytics Engine service - https://cloud.ibm.com/catalog/services/analytics-engine

### Training Application - Data Load
- Create an account on Github and share it with your supervisor
- Create an instance of DB2 service on the cloud
- Create a Scala application that
  - Connects to the DB2 instance on the cloud
  - Generates a table with 20k of sample records
    - *product_id* - autogenerated numeric
    - *product_group* - autogenerated numeric in 0..9 range
    - *year* - autogenerated numeric in 2015..2018 range
    - 12 columns with monthly purchases amount - numeric in 0..100000 range
  - There should be no duplicates for the same product/year
  - If there are multiple product rows for different years, product/group combination should be concise
- Application should be built using **sbt** tool

### Training Application - Data Transformation
- Create a data tranformation application using Apache Spark
- Read data from the DB2 on the cloud (use the service instance and data from the previous task) 
- Aggregate data in the dataframe by calculating total purchases amount per year
  - For each row calculate total of monthly purchases
  - Save the year total as a new column *year_purchases*
  - Remove the columns with monthly amounts from the data frame
- Save the modified dataframe as a file in Cloud Object Storage - https://www.ibm.com/cloud/object-storage
  - Use Stocator library - https://github.com/CODAIT/stocator
- Application should be built using **sbt** tool and published to Github

### Training Application - Data Partitioning
- Modify the data transformation application to support data partitioning
  - Spark Documentation - https://spark.apache.org/docs/2.2.1/sql-programming-guide.html#jdbc-to-other-databases
- Partition data by *year* value and play with number of executors/workers to see the effects
- Useful links
  - Tips for using JDBC in Apache Spark SQL - https://medium.com/@radek.strnad/tips-for-using-jdbc-in-apache-spark-sql-396ea7b2e3d3
  - How to optimize partitioning when migrating data from JDBC source? - https://stackoverflow.com/questions/52603131/how-to-optimize-partitioning-when-migrating-data-from-jdbc-source
  - How to improve performance for slow Spark jobs using DataFrame and JDBC connection? - https://stackoverflow.com/questions/32188295/how-to-improve-performance-for-slow-spark-jobs-using-dataframe-and-jdbc-connecti
- Application should be built using **sbt** tool and published to Github

### Training Application - Unit Tests
- Add unit tests to the training application
  - For unit tests creation use *ScalaTest* framework - http://www.scalatest.org/user_guide
  - Tests style *org.scalatest.FunSpec* - http://www.scalatest.org/getting_started_with_fun_spec
  - Mocks for external connections *org.scalamock.scalatest.MockFactory* - https://scalamock.org/
  - Utility to compare dataframes *com.github.mrpowers.spark.fast.tests.DataFrameComparer* - https://github.com/MrPowers/spark-fast-tests
- Include unit tests into the **sbt** build script

## Docker + Kubernetes
- Complete the following courses
  - Docker Essentials: A Developer Introduction - https://cognitiveclass.ai/courses/docker-essentials/
  - Container & Kubernetes Essentials with IBM Cloud - https://cognitiveclass.ai/courses/kubernetes-course/
  
## Spark + Kubernetes
- Study the Apache Spark documentation
  - Running Spark on Kubernetes - https://spark.apache.org/docs/latest/running-on-kubernetes.html
  
### Apache Airflow
- Familiarize yourself with Apache Airflow - https://airflow.apache.org/
- Create a sample DAG for a Spark submit and schedule its execution
- Create a Python script to create the same DAG programatically via Airflow API
